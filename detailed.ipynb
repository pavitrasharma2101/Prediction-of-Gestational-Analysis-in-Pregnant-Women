{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, precision_recall_curve\n",
    "\n",
    "# Load the dataset\n",
    "df_diabetes = pd.read_csv('Data.csv')\n",
    "\n",
    "# Checking the shape of the dataframe\n",
    "print(\"Data Shape:\", df_diabetes.shape)\n",
    "\n",
    "# Display the first few rows of the dataset\n",
    "print(\"First 5 Rows:\")\n",
    "print(df_diabetes.head())\n",
    "\n",
    "# Check data types of columns\n",
    "print(\"Data Types:\")\n",
    "print(df_diabetes.dtypes)\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing Values:\")\n",
    "print(df_diabetes.isnull().sum())\n",
    "\n",
    "# Visualize the count of the 'Prediction' column\n",
    "ax = sns.countplot(df_diabetes['Prediction'])\n",
    "ax.yaxis.grid()\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_linewidth(False)\n",
    "ax.spines['left'].set_linewidth(False)\n",
    "plt.show()\n",
    "\n",
    "# Remove 'Prediction' column for data preprocessing\n",
    "df_diabetes1 = df_diabetes.drop('Prediction', axis=1, inplace=False)\n",
    "\n",
    "# Histograms of the dataset\n",
    "ax = df_diabetes1.hist(figsize=(10, 10))\n",
    "plt.show()\n",
    "\n",
    "# Pairplot to visualize relationships in the dataset\n",
    "ax = sns.pairplot(df_diabetes, hue='Prediction')\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "y = df_diabetes['Prediction']\n",
    "X = df_diabetes.drop('Prediction', axis=1, inplace=False)\n",
    "\n",
    "# Standardize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X = pd.DataFrame(X, columns=['Age', 'Pregnancy No', 'Weight', 'Height', 'BMI', 'Heredity'])\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Logistic Regression model\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, y_train)\n",
    "y_pred_logreg = logreg.predict(X_test)\n",
    "\n",
    "# Decision Tree model\n",
    "dectree = DecisionTreeClassifier()\n",
    "dectree.fit(X_train, y_train)\n",
    "y_pred_dectree = dectree.predict(X_test)\n",
    "\n",
    "# K-Nearest Neighbors model\n",
    "neighbors = np.arange(1, 15)\n",
    "train_accuracy = np.empty(len(neighbors)\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "\n",
    "for i, k in enumerate(neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    train_accuracy[i] = knn.score(X_train, y_train)\n",
    "    test_accuracy[i] = knn.score(X_test, y_test)\n",
    "\n",
    "# Plot k-NN Accuracy for different numbers of neighbors\n",
    "plt.title('k-NN Accuracy for different number of neighbors')\n",
    "plt.plot(neighbors, test_accuracy, label='Testing Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label='Training accuracy')\n",
    "plt.legend()\n",
    "plt.xlabel('Number of neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()\n",
    "\n",
    "# K-Nearest Neighbors model with k=13\n",
    "knn = KNeighborsClassifier(n_neighbors=13)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred_knn = knn.predict(X_test)\n",
    "\n",
    "# Random Forest model\n",
    "ranfor = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "ranfor.fit(X_train, y_train)\n",
    "y_pred_ranfor = ranfor.predict(X_test)\n",
    "\n",
    "# AdaBoost model\n",
    "abc = AdaBoostClassifier(n_estimators=1000)\n",
    "abc.fit(X_train, y_train)\n",
    "y_pred_abc = abc.predict(X_test)\n",
    "\n",
    "# Voting Classifier without weights\n",
    "vc = VotingClassifier(estimators=[('logreg', logreg), ('dectree', dectree), ('ranfor', ranfor), ('knn', knn), ('abc', abc)], voting='soft')\n",
    "vc.fit(X_train, y_train)\n",
    "y_pred_vc = vc.predict(X_test)\n",
    "\n",
    "# Voting Classifier with weights\n",
    "vc1 = VotingClassifier(estimators=[('logreg', logreg), ('dectree', dectree), ('ranfor', ranfor), ('knn', knn), ('abc', abc)], voting='soft', weights=[2, 1, 2, 2, 1])\n",
    "vc1.fit(X_train, y_train)\n",
    "y_pred_vc1 = vc1.predict(X_test)\n",
    "\n",
    "# Model Accuracy\n",
    "print('Model Accuracy')\n",
    "print('\\n')\n",
    "print('Logistic Regression:', round(accuracy_score(y_test, y_pred_logreg) * 100, 2), '%')\n",
    "print('Decision Tree:', round(accuracy_score(y_test, y_pred_dectree) * 100, 2), '%')\n",
    "print('KNN:', round(accuracy_score(y_test, y_pred_knn) * 100, 2), '%')\n",
    "print('\\n')\n",
    "print('Averaging Method')\n",
    "print('Random Forest:', round(accuracy_score(y_test, y_pred_ranfor) * 100, 2), '%')\n",
    "print('\\n')\n",
    "print('Boosting Method')\n",
    "print('AdaBoost:', round(accuracy_score(y_test, y_pred_abc) * 100, 2), '%')\n",
    "print('\\n')\n",
    "print('Voting Classifiers')\n",
    "print('Voting Classifier without Weights:', round(accuracy_score(y_test, y_pred_vc)))\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
